---
title: "Web Scraping"
output:
  html_document:
    df_print: paged
---


```{r message=FALSE, warning=FALSE}
# Load packages
library(tidyverse)  
library(tibble)
library(tidyr)
library(rvest)
library(dplyr)
library(stringr)
library(purrr)
library(lubridate)
```


```{r}
scrape_page_info <- function(page_url){
  
  html <- read_html(page_url)
  
  topics <- html %>%
    html_nodes(".title a") %>%
    html_text()
  
  topic_urls <- html %>%
    html_nodes(".title a") %>%
    html_attr(name = "href") %>%
    paste0("https://forums.tesla.com", .)
  
  created_info <- html %>%
    html_nodes(".created") %>%
    html_text() %>%
    str_squish()
  
  tibble(topics, created_info, topic_urls) %>%
    separate(col = created_info, into = c("date_of_creation", "thread_author"), sep = " by ")
  
}
```



```{r}
scrape_thread_info <- function(thread_html){
  
  thread_html %>%
    html_nodes(".clearfix") %>%
    html_text() %>%
    str_squish() %>%
    str_replace(pattern = "^(.*?(\\|.*?){1})\\|", replacement = "\\1") %>%  # Remove second "|"
    str_replace(pattern = "(^.*?\\d{4})", replacement = "\\1 \\|") %>% # Add "|" after first date
    enframe(name = NULL, value = "content") %>%
    separate(col = "content", into = c("author", "date", "content"), sep = "\\|")
  
}
```



```{r}
scrape_thread <- function(url){
  
  html <- read_html(url)
  
  n_pages <- html %>%
    html_node("#article_content > div.panel-pane.pane-node-comments > div > div.item-list > ul > li.pager-last.last > a") %>%
    html_attr("href") %>%
    str_extract(pattern = "(\\d+)$") %>%
    as.numeric()
  
  df_page_1 <-
    html %>%
    html_nodes(".clearfix") %>%
    html_text() %>%
    str_squish() %>%
    str_replace(pattern = "^(.*?(\\|.*?){1})\\|", replacement = "\\1") %>%  # Remove second "|"
    str_replace(pattern = "(^.*?\\d{4})", replacement = "\\1 \\|") %>% # Add "|" after first date
    enframe(name = NULL, value = "content") %>%
    separate(col = "content", into = c("author", "date", "content"), sep = "\\|")
  
  df_page_1$author[1] <- html %>%
    html_node(".username") %>%
    html_text()
  
  df_page_1$date[1] <- html %>%
    html_node(".submitted") %>%
    html_text() %>%
    str_squish() %>%
    str_extract(pattern = "\\w+\\s\\d+\\W\\s\\d{4}$")
  
  df_page_1$content[1] <- html %>%
    html_node(".clearfix") %>%
    html_text() %>%
    str_squish() %>%
    str_replace(pattern = "^.*\\d+\\,\\s\\d{4}\\s", replacement = "")
    
  extra_page_data <- NULL
  
  if(!is.na(n_pages)){
    extra_urls <- paste0(thread_url, "p", seq_len(n_pages-1))
    other_htmls <- lapply(extra_urls, function(x) read_html(x))
    df <- lapply(other_htmls, function(x){
      scrape_thread_info(x)[-1, ]
    })
    extra_page_data <- do.call(rbind, df)
  } 
  
  rbind(df_page_1, extra_page_data)
  
}
```



```{r}
scrape_thread_possibly <- possibly(scrape_thread, otherwise = NA)

# Actual scraping

page_urls <- c("https://forums.tesla.com/categories/tesla-model-3", 
               paste0("https://forums.tesla.com/categories/tesla-model-3/p", 1:2))

master_data <- map_dfr(page_urls[1:2], function(url){
  scrape_page_info(url) %>%
    mutate(forum_data = map(topic_urls, scrape_thread_possibly))
})
```


html <- read_html(page_url)
  
  topics <- html %>%
    html_nodes(".title a") %>%
    html_text()
    
# Lego

```{r}
html <- read_html("https://forums.tesla.com/discussion/59386/")
```

#main-body .Comments

```{r}
subject <- html %>%
  html_nodes("#main-body .Discussion") %>%
  html_text()
subject <- tibble(Subject = subject)
subject
```

XPATH
xpath = "//html/body/div[8]/main/ul/li[1]/div/div[2]/a"

CSS Path
html.wf-opensans-n4-active.wf-opensans-i4-active.wf-opensans-n6-active.wf-opensans-n7-active.wf-active body#vanilla_categories_index.Vanilla.Categories.isDesktop.index.Discussions.Category-tesla-model-3.Section-Category-tesla-model-3.Section-DiscussionList.tds-header-transparent--dark.js-focus-visible div#main-body.tds-layout.tds-layout-3col-has_main--inset main.tds-layout-item.tds-layout-main ul.DataList.Discussions li#Discussion_59386.Item.Announcement.Announcement-Category.Unread.ItemDiscussion.noPhotoWrap.ItemDiscussion-withPhoto div.ItemContent.Discussion div.Title.discussion--title a

CSS Selector
"#Discussion_59386 > div:nth-child(3) > div:nth-child(2) > a:nth-child(1)"

```{r}
urls <- html %>%
  html_nodes(xpath = "//html/body/div[8]/main/div/div[2]") %>%
  html_text()
urls

```



```{r}
topics <- html %>% 
  html_nodes(".userContent") %>%
  html_text()

topics <- tibble(Topics = topics)
topics
```

```{r}
pager <- html %>% 
  html_nodes(".LastPage") %>%
  html_text()
pager
```


```{r}
topics <- html %>% 
  html_nodes("#main-body") %>%
  html_text()
topics <- tibble(Topics = topics)
topics
```




# Main Page

```{r}
html <- read_html("https://forums.tesla.com/categories/tesla-model-3")
```



```{r}
topic_urls <- html %>%
  html_nodes(xpath = '//html/body/div/main/ul/li/div/div[1]/a') %>%
  html_attr("href")

topics <- html %>%
  html_nodes(xpath = '//html/body/div/main/ul/li/div/div[1]') %>%
  html_text(trim = TRUE)

main_topics <- tibble(
  Topic = topics,
  URLs = topic_urls
)

main_topics
```

```{r}
topic_last_page <- html %>%
  html_node(".LastPage") %>%
  html_text()
topic_last_page
```







```{r}
topics <- html2 %>% 
  html_nodes(".ItemDiscussion-withPhoto") %>%
  html_text()
topics <- tibble(Topics = topics)
topics
```

```{r}
topics <- html2 %>% 
  html_nodes("a") %>%
  html_text()
topics <- tibble(Topics = topics)
topics
```








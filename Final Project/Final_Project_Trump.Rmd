---
title: "Assignment 13"
author: "Brian Roepkee"
date: "Dec 6, 2020"
output:
  html_document:
    df_print: paged
---
# Sentiment Analysis

Sentiment analysis of Twitter Data..

## Load Libraries and Data

```{r message=FALSE, warning=FALSE}
# twitter library 
library(rtweet)

# plotting and pipes
library(tidyverse)
library(ggplot2)
library(dplyr)
library(stringr)
library(tidyr)
library(ggthemes)

# text mining library
library(tm)
library(tidytext)
library(wordcloud)
library(reshape2)
library(textstem)
library(ggraph)
library(igraph)
library(widyr)

# date/time library
library(lubridate)
```


```{r}
df <- read_twitter_csv("trump_rally.csv")
```


```{r}
# Convert the date field to a datetime
df$created_at <- as_datetime(df$created_at)

#Changed some fields to factors for easier manipulation later
df$user_id <- as.factor(df$user_id)
df$status_id <- as.factor(df$status_id)
df$screen_name <- as.factor(df$screen_name)

# Fix up the reply count field.  It should be a int and NAs set to 0
df$reply_count[is.na(df$reply_count)] <- 0
df$reply_count <- as.integer(df$reply_count)
```


```{r}
head(df)
```


```{r}
tail(df)
```

### Filter out Retweets

Validate the number of rows before and after removing retweets.

```{r}
nrow(df)
```


```{r}
df <- df %>%
  filter(is_retweet == FALSE)
head(df)
```

Show the total number of rows in this dataset.

```{r}
nrow(df)
```

### Add Doc_Id incrementing per Row

```{r}
df = df %>%
  mutate(doc_id = paste0("doc", row_number())) %>%
  select(doc_id, everything())
head(df)
```

### Add a Column for Text Length

```{r}
df$text_len <- str_count(df$text)
```

# EDA

Simplify the Data Frame to just the target fields we'll be using throughout the analysis.

```{r}
df_select <- df %>%
  select(doc_id, user_id, status_id, screen_name, created_at, text, text_len)
```

## Tweet Length
Number of tweets by each candidate

```{r}
summary(df_select)
```

Tweets for the dataset range from `7` characters to `316` with a mean of `172`



```{r}
df_select %>%
  ggplot(aes(x = text_len, fill="#1F77B4")) +
  geom_histogram(bins = 32, show.legend = FALSE) +
  theme_minimal() +
  scale_fill_tableau()
```


### Tweets Over Time

```{r}
df_select %>%
  ts_plot("1 minute", trim = 0L) +
  geom_line(color = "#1F77B4") +
  theme_minimal() +
  scale_fill_tableau() +
  theme(
    legend.title = element_blank(),
    legend.position = "right",
    plot.title = element_text(face = "bold")) +
  labs(
    x = NULL, y = NULL,
    title = "Frequency of Tweets"
  )
```

# Clean and Prepare the Text for Analysis

Get rid of various useless text like URLS and shortened URLs.  These appear frequently in the text and skew results.

### Clean Twitter specific items
https://stackoverflow.com/questions/31348453/how-do-i-clean-twitter-data-in-r


```{r}
df_select$text = gsub("[ \t]{2,}", " ", df_select$text)
df_select$text = gsub("http\\t.c+", " ", df_select$text)
df_select$text = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", df_select$text)
df_select$text = gsub("@\\w+", " ", df_select$text)
df_select$text = gsub("http[[:alnum:][:punct:]]*", " ", df_select$text)
df_select$text = gsub("^\\s+|\\s+$", " ", df_select$text)
df_select$text = gsub("[[:digit:]]", " ", df_select$text)
df_select$text <- str_replace_all(df_select$text," "," ")
df_select$text <- str_replace(df_select$text,"RT @[a-z,A-Z]*: ","")
df_select$text <- str_replace_all(df_select$text,"#[a-z,A-Z]*","")
df_select$text <- str_replace_all(df_select$text,"@[a-z,A-Z]*","") 
# Non Latin Characters
df_select$text <- iconv(df_select$text, "latin1", "ASCII", sub = "")
# New Lines
df_select$text <- str_replace_all(df_select$text,"\\n","") 
df_select$text <- str_replace_all(df_select$text,'\"',"") 
# Ampersands
df_select$text <- gsub("&amp", "", df_select$text)
```


### Remove Whitespace and Lemmatize

```{r}
df_select$text = stripWhitespace(df_select$text)
df_select$text = lemmatize_strings(df_select$text)
```


```{r}
tail(df_select$text, n=10)
```


### Unnest_Tokens()

Create a new column with each word on it's own row.

```{r}
tidy_df <- df_select %>%
  unnest_tokens(word, text)
```

### Stop Word Removal
remove stop words and custom stop words from the results.

```{r}
custom_stop_words <- bind_rows(tibble(word = c("fuck", "shit", "trump"), lexicon = c("custom")), stop_words)

tidy_df <- tidy_df %>%
  anti_join(custom_stop_words, by = "word")
```

### Validate the New number of Rows
Dramatically larger now that each word from text is in it's own row.

```{r}
nrow(tidy_df)
```


# Sentiment Analysis

## Bing Sentiment Lexicon
Using the Bing Lexicon from Bing Liu and collaborators, adds the column "Sentiment" and mark each word as positive or negative. 

https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html

```{r}
bing_df <- tidy_df %>%
  inner_join(get_sentiments("bing"), by = "word")
```


```{r}
bing_df %>%
  group_by(sentiment) %>%
  summarise(count = n(), .groups = "keep")
```


### AFINN scoring Lexicon
AFINN from Finn Ã…rup Nielsen, adds the `value` column, with a numeric representation of how positive, or negative the word is.  The AFINN lexicon measures sentiment with a numeric score between -5 and 5

http://www2.imm.dtu.dk/pubdb/pubs/6010-full.html 

```{r}
afinn_df <- tidy_df %>%
  inner_join(get_sentiments("afinn"), by = "word")

head(afinn_df)
```


```{r}
afinn_df %>%
  ggplot(aes(x = value, fill="#1F77B4")) +
  geom_histogram(bins = 10, show.legend = FALSE) +
  scale_x_continuous(breaks = c(-5, -3, -1, 1, 3, 5)) +
  theme_minimal() +
  scale_fill_tableau()
```

For the dataset overall, there is a slight left-skew showing there is a greater concentration of words with positive values.  There are very few in the high and low values (`-4`,`-5`, `+5`). 

*Note: `0` is not a valid value in this scoring system, therefore the bin is empty*

### NRC Sentiment Lexicon
NRC from Saif Mohammad and Peter Turney. The NRC Emotion Lexicon is a list of English words and their associations with eight basic emotions as well as positive and negative sentiment.

One thing to note, single words can have multiple emotions

```{r}
nrc_df <- tidy_df %>%
  inner_join(get_sentiments("nrc"), by = "word")
```

Total counts for all 8 emotions and 2 sentiments.

```{r}
nrc_df %>%
  group_by(sentiment) %>%
  summarise(total = n(), .groups = "keep") %>%
  arrange(desc(total))
```

## Inspect Top Words Per Candidate
Using various methods, inspect what words are most frequently used, per candidate, proportions of negative and positive words, and trend over time. 

### Top Word Counts (BING)

```{r message=FALSE, warning=FALSE}
bing_df %>%
  count(word, sort = TRUE, sentiment) %>%

  group_by(sentiment) %>%
  top_n(15) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  theme_minimal() +
  scale_fill_tableau() +
  labs(x = "Contribution to sentiment",
       y = NULL)
```

When looking at data as a whole, we can see the top negative word is `crisis` and positive is `win`.  Next, we'll split these out by candidate. 

### Overall Top Words (BING)

```{r message=FALSE, warning=FALSE}
bing_df %>%
  count(word, sort = TRUE, sentiment) %>%
  top_n(40) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word, fill = sentiment)) +
  geom_col(show.legend = TRUE) +
  theme_minimal() +
  scale_fill_tableau() +
  labs(x = "Contribution to sentiment", y = NULL)
```


### Top Words Sorted by AFINN Score

```{r message=FALSE, warning=FALSE}
afinn_df$color <- ifelse(afinn_df$value < 0, "Negative","Positive")

afinn_df %>%
  count(word, sort = TRUE, value, color) %>%
  top_n(30) %>%
  ungroup() %>%
  mutate(word = reorder(word, value)) %>%
  
  ggplot(aes(value, word, fill = color)) +
  geom_col(show.legend = TRUE) +
  theme_minimal() +
  scale_fill_tableau() +
  labs(x = "AFINN Sentiment Score", y = NULL)
```


### Sentiment of Tweets over Time (AFINN)

```{r}
plot_df2 <- afinn_df %>%
  mutate(mon = floor_date(created_at, "30 minutes")) %>%
  group_by(screen_name, mon) %>%
  summarize(value = mean(value), .groups = 'keep')

plot_df2$color <- ifelse(plot_df2$value < 0, "negative","positive")

ggplot(plot_df2, aes(mon, value, fill = color)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "Mean AFINN Sentiment Score") +
  theme_minimal() +
  scale_fill_tableau()
```

Since August, showing the frequency of sentiment for each candidate.  Using the mean sentiment score we can see how each candidate tweeted on a daily basis.  Trumps earlier timeline tended to be more negative, while both candidates were more positive starting mid September. 


## Most Positive Messages

```{r}
afinn_df %>%
  group_by(doc_id) %>%
  summarize(total_value = sum(value), word_count = n(), .groups = "keep") %>%
  arrange(desc(total_value)) %>%
  head()
```

```{r}
df %>%
  filter(doc_id == "doc6458" | doc_id == "doc7806" | doc_id == "doc9199") %>%
  select(text)
```


## Most Negative Messages

```{r}
afinn_df %>%
  group_by(doc_id) %>%
  summarize(total_value = sum(value), word_count = n(), .groups = "keep") %>%
  arrange(total_value) %>%
  head()
```

```{r}
df %>%
  filter(doc_id == "doc3517" | doc_id == "doc2020" | doc_id == "doc12965") %>%
  select(text)
```

# Word Clouds

## Overall Word Cloud

```{r message=FALSE, warning=FALSE}
bing_df %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 150))
```


## By Sentiment
Word cloud of the top 200 words grouped by sentiment, positive or negative.

```{r message=FALSE, warning=FALSE}
bing_df %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("#1F77b4", "#FF7F0E"), max.words = 200)
```


# Checking for Top-Negation Word Pairs
The method used here doesn't take into account negation of words like no, and never preceding words.  We can quckly check the top word pairs to see how they occur in our text.

```{r}
df_select$text <- removeWords(df_select$text, c(stopwords('english'), "i"))
```


```{r}
bigrams <- df_select %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)
```


```{r}
bigram_counts <- bigrams %>%
  count(bigram, sort = TRUE) %>%
  separate(bigram, c("word1", "word2"), sep = " ")

head(bigram_counts, n=10)
```


```{r}
negate_words <- c("not", "without", "no", "can't", "don't", "won't")


bigram_counts %>%
  filter(word1 %in% negate_words) %>%
  count(word1, word2, wt = n, sort = TRUE) %>%
  inner_join(get_sentiments("afinn"), by = c(word2 = "word")) %>%
  mutate(contribution = value * n) %>%
  group_by(word1) %>%
  slice_max(abs(contribution), n = 10) %>%
  ungroup() %>%
  mutate(word2 = reorder_within(word2, contribution, word1)) %>%

  ggplot(aes(contribution, word2, fill = contribution > 0)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~word1, scales = "free", nrow = 2) +
  scale_y_reordered() +
  theme_minimal() +
  scale_fill_tableau() +
  labs(x = "Sentiment value * # of occurrences",
       y = "Words preceded by a negation")
```



```{r message=FALSE, warning=FALSE}
bigram_graph <- bigram_counts %>%
  filter(n > 20) %>%
  graph_from_data_frame()

set.seed(2017)

ggraph(bigram_graph, layout = "fr") +
  geom_edge_link() +
  geom_node_point() +
  geom_node_text(aes(label = name), vjust = -.5, hjust = -.2) +
  theme_minimal() +
  scale_fill_tableau()
```


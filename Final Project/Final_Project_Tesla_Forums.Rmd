---
title: "Data 104 Final Project"
author: "Brian Roepkee"
date: "Dec 20, 2020"
output:
  html_document:
    df_print: paged
    code_folding: hide
    toc: true
---

# Tesla Model 3 Discussion Forum Analysis



```{r message=FALSE, warning=FALSE}
# twitter library 
library(rtweet)

# plotting and pipes
library(tidyverse)
library(stringr)
library(tidyr)
library(ggthemes)

# text mining library
library(tm)
library(tidytext)
library(wordcloud)
library(reshape2)
library(textstem)
library(ggraph)
library(igraph)
library(widyr)
library(spacyr)
library(SnowballC)
library(topicmodels)
library(quanteda)
library(seededlda)

# Machine Learning
library(caret)


# date/time library
library(lubridate)
```


```{r}
df <- read.csv('tesla_forums.csv')
```

Adjust any variable formats as needed.

```{r}
df$Time <- as.Date(df$Time, "%m/%d/%y")
df$User <- as.factor(df$User)
df$Topic <- as.factor(df$Topic)
```


```{r}
str(df)
```


```{r}
summary(df)
```

```{r}
df <- drop_na(df)
```

Removed all duplicates.  The scraping method used seemed to create quite a few.

```{r}
df <- distinct(df)
```


```{r}
summary(df)
```


```{r}
df_topics <- df %>%
  group_by(Topic) %>%
  summarise(count = n(), .groups="keep") %>%
  arrange(desc(count))
head(df_topics)
```
Remove the first topic, it's just the "how to use the forums" thread and doesn't aid in analysis

```{r}
df <- df[-c(1:24), ]
```


```{r}
sprintf("There are %s unique topics", nrow(df_topics))
sprintf("There are %s total discussion threads (replies)", nrow(df))
```

### Add Doc_Id incrementing per Row

```{r}
df = df %>%
  mutate(doc_id = paste0("doc", row_number())) %>%
  select(doc_id, everything())
```

### Add a Column for Text Length

```{r}
df$text_len <- str_count(df$Discussion)
```

# EDA

Make a copy of the original DF so it can be referenced later.

```{r}
df_select <- df
```




## Tweet Length
Number of tweets by each candidate

```{r}
summary(df_select$text_len)
```

Tweets for the dataset range from `0` characters to `316` with a mean of `7470` with a mean of `284`. 


```{r warning=FALSE}
df_select %>%
  ggplot(aes(x = text_len)) +
  geom_histogram(bins = 50, fill="lightgray", color="darkgray") +
  theme_minimal() +
  scale_y_log10() +
  scale_colour_grey(start = 0.3, end = .8)
```



```{r warning=FALSE}
plot_df_time <- df %>%
  mutate(date = floor_date(Time, "week")) %>%
  group_by(date) %>%
  summarize(count = n(), .groups='keep')


ggplot(plot_df_time, aes(date, count)) +
  geom_line(show.legend = FALSE) +
  labs(x = NULL, y = "") +
  theme_minimal()
```



## Outlier Analysis

```{r}
df_select %>%
  filter(text_len > 5000) %>%
  select(Discussion) %>%
  head(n=1)
```

**Note:** Since this is discussion forum text, outliers are simply long posts as demonstrated above.  They will remain in the dataset since they are valuable information.

### Discussion Timeline

# Clean and Prepare the Text for Analysis

### Clean The Text

```{r}
df_select$Discussion <- iconv(df_select$Discussion, "latin1", "ASCII", sub = "")
df_select$Discussion <- str_replace_all(df_select$Discussion,"\\n","")
df_select$Discussion <- str_replace_all(df_select$Discussion,"@","")
df_select$Discussion <- gsub("http[[:alnum:][:punct:]]*", "", df_select$Discussion)
```


### Remove Whitespace, Punctuation, Stopwords and Lemmatize

```{r}
df_select$Discussion = removePunctuation(df_select$Discussion)
df_select$Discussion = stripWhitespace(df_select$Discussion)
df_select$Discussion = tolower(df_select$Discussion)
df_select$Discussion = removeWords(df_select$Discussion, c(stopwords('english')))
df_select$Discussion = lemmatize_strings(df_select$Discussion)
```


```{r}
head(df_select$Discussion)
```


### Unnest_Tokens()

Create a new column with each word on it's own row.

```{r}
tidy_df <- df_select %>%
  unnest_tokens(word, Discussion)
```

### Validate the New number of Rows
Dramatically larger now that each word from text is in it's own row.

```{r}
nrow(tidy_df)
```

# Sentiment Analysis

## Bing Sentiment Lexicon
Using the Bing Lexicon from Bing Liu and collaborators, adds the column "Sentiment" and mark each word as positive or negative. 

https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html

```{r}
bing_df <- tidy_df %>%
  inner_join(get_sentiments("bing"), by = "word")
```


```{r}
bing_df %>%
  group_by(sentiment) %>%
  summarise(count = n(), .groups = "keep")
```


### AFINN scoring Lexicon
AFINN from Finn Ã…rup Nielsen, adds the `value` column, with a numeric representation of how positive, or negative the word is.  The AFINN lexicon measures sentiment with a numeric score between -5 and 5

http://www2.imm.dtu.dk/pubdb/pubs/6010-full.html 

```{r}
afinn_df <- tidy_df %>%
  inner_join(get_sentiments("afinn"), by = "word")

head(afinn_df)
```


```{r}
afinn_df %>%
  ggplot(aes(x = value)) +
  geom_histogram(bins = 10, show.legend = FALSE, fill="lightgray", color="darkgray") +
  scale_x_continuous(breaks = c(-5, -3, -1, 1, 3, 5)) +
  theme_minimal() +
  scale_colour_grey(start = 0.3, end = .8)
```

For the dataset overall, there is a slight left-skew showing there is a greater concentration of words with positive values.  There are very few in the high and low values (`-4`,`-5`, `+5`). 

*Note: `0` is not a valid value in this scoring system, therefore the bin is empty*

### NRC Sentiment Lexicon
NRC from Saif Mohammad and Peter Turney. The NRC Emotion Lexicon is a list of English words and their associations with eight basic emotions as well as positive and negative sentiment.

One thing to note, single words can have multiple emotions

```{r}
nrc_df <- tidy_df %>%
  inner_join(get_sentiments("nrc"), by = "word")
```

Total counts for all 8 emotions and 2 sentiments.

```{r}
nrc_df %>%
  group_by(sentiment) %>%
  summarise(total = n(), .groups = "keep") %>%
  arrange(desc(total))
```

## Inspect Top Words Per Candidate
Using various methods, inspect what words are most frequently used, per candidate, proportions of negative and positive words, and trend over time. 

### Top Word Counts (BING)

```{r message=FALSE, warning=FALSE}
bing_df %>%
  count(word, sort = TRUE, sentiment) %>%

  group_by(sentiment) %>%
  top_n(15) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free") +
  theme_minimal() +
  labs(x = "Contribution to sentiment",
       y = NULL)
```

When looking at data as a whole, we can see the top negative word is `crisis` and positive is `win`.  Next, we'll split these out by candidate. 

### Overall Top Words (BING)

```{r message=FALSE, warning=FALSE}
bing_df %>%
  count(word, sort = TRUE, sentiment) %>%
  top_n(30) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word, fill = sentiment)) +
  geom_col(show.legend = TRUE) +
  theme_minimal() +
  labs(x = "Contribution to sentiment", y = NULL)
```


### Top Words Sorted by AFINN Score

```{r message=FALSE, warning=FALSE}
afinn_df$color <- ifelse(afinn_df$value < 0, "Negative","Positive")

afinn_df %>%
  count(word, sort = TRUE, value, color) %>%
  top_n(30) %>%
  ungroup() %>%
  mutate(word = reorder(word, value)) %>%
  ggplot(aes(value, word, fill = color)) +
  geom_col(show.legend = TRUE) +
  theme_minimal() +
  labs(x = "AFINN Sentiment Score", y = NULL)
```


### Sentiment over Time (AFINN)

```{r warning=FALSE}
plot_df2 <- afinn_df %>%
  mutate(mon = floor_date(Time, "month")) %>%
  group_by(mon) %>%
  summarize(value = mean(value), .groups = 'keep')

plot_df2$color <- ifelse(plot_df2$value < 0, "negative","positive")

ggplot(plot_df2, aes(mon, value, fill = color)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "Mean AFINN Sentiment Score") +
  theme_minimal()
```

Since August, showing the frequency of sentiment for each candidate.  Using the mean sentiment score we can see how each candidate tweeted on a daily basis.  Trumps earlier timeline tended to be more negative, while both candidates were more positive starting mid September. 


## Most Positive Messages

```{r}
afinn_df %>%
  group_by(doc_id) %>%
  summarize(total_value = sum(value), word_count = n(), .groups = "keep") %>%
  arrange(desc(total_value)) %>%
  head()
```

```{r}
df %>%
  filter(doc_id == "doc26365" | doc_id == "doc30589" | doc_id == "doc20143") %>%
  select(Discussion)
```


## Most Negative Messages

```{r}
afinn_df %>%
  group_by(doc_id) %>%
  summarize(total_value = sum(value), word_count = n(), .groups = "keep") %>%
  arrange(total_value) %>%
  head()
```

```{r}
df %>%
  filter(doc_id == "doc11357" | doc_id == "doc35860" | doc_id == "doc34072") %>%
  select(Discussion)
```

# Word Clouds

## Overall Word Cloud

```{r message=FALSE, warning=FALSE}
bing_df %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 250))
```


## By Sentiment
Word cloud of the top 200 words grouped by sentiment, positive or negative.

```{r message=FALSE, warning=FALSE}
bing_df %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("gray", "black"), max.words = 150)
```


# n-Grams

## Bi-Grams

```{r}
bigrams <- df_select %>%
  unnest_tokens(bigram, Discussion, token = "ngrams", n = 2)
```


```{r}
bigram_counts <- bigrams %>%
  count(bigram, sort = TRUE) %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigram_counts <- drop_na(bigram_counts)
bigram_counts
```


```{r message=FALSE, warning=FALSE}
bigram_graph <- bigram_counts %>%
  filter(n > 250) %>%
  graph_from_data_frame()

set.seed(2017)
a <- grid::arrow(type = "open", length = unit(.05, "inches"))

ggraph(bigram_graph, layout = "nicely") +
  geom_edge_link(arrow = a, end_cap = circle(.02, 'inches')) +
  geom_node_point(color = "gray", size = 2) +
  geom_node_text(aes(label = name), vjust = -1, hjust = 1) +
  theme_minimal()
```

## Trigrams

```{r}
trigrams <- df_select %>%
  unnest_tokens(bigram, Discussion, token = "ngrams", n = 3)
```


```{r}
trigram_counts <- trigrams %>%
  count(bigram, sort = TRUE) %>%
  separate(bigram, c("word1", "word2", "word3"), sep = " ")

trigram_counts <- drop_na(trigram_counts)
trigram_counts
```


```{r message=FALSE, warning=FALSE}
trigram_graph <- trigram_counts %>%
  filter(n > 50) %>%
  graph_from_data_frame()

set.seed(2017)
a <- grid::arrow(type = "open", length = unit(.05, "inches"))

ggraph(trigram_graph, layout = "nicely") +
  geom_edge_link(arrow = a, end_cap = circle(.02, 'inches')) +
  geom_node_point(color = "gray", size = 2) +
  geom_node_text(aes(label = name), vjust = 1.5, hjust = -.25) +
  theme_minimal()
```

# Topic Modeling

## Preprocessing

```{r}
corpus <- Corpus(VectorSource(df_select$Discussion))
```

### Before cleaning

```{r}
inspect(corpus[1:5])
```

### Remove Words that Aren't Helpful for Topic Modeling

Custom list of words generated upon performing the topic modeling.  These were frequently appeared but do not add a lot of context to topic identification, or appear so frequently, such as `University`, that they are in every topic. 

```{r}
corpus <- tm_map(corpus, removeWords, c("tesla", "get", "much", "can", 
                                        "will", "say", "car", "may", "use",
                                        "just", "one", "good", "like", "think"))
```




```{r}
inspect(corpus[1:10])
```


### Document Term Matrix Creation

```{r}
dtm <- DocumentTermMatrix(corpus)
```


```{r}
inspect(dtm)
```

### Remove Sparse Terms

```{r}
dtm = removeSparseTerms(dtm, .995)
inspect(dtm)
```



```{r}
sel_idx <- rowSums(as.matrix(dtm)) > 0
dtm <- dtm[sel_idx, ]
dim(dtm)
```

## Latent Dirichlet Allocation (LDA)
Is an unsupervised classification method designed for text data. Rather than relying on labeled (eg: hand-coded data sets), it is a probabilistic topic model that uses statistical algorithms to analyze words in raw text documents, to uncover thematic structure of the both the corpus and individual documents.

### Find the best k - the Elbow Method
The basic idea behind partitioning methods, such as k-means clustering, is to define clusters such that the total intra-cluster variation [or total within-cluster sum of square (WSS)] is minimized. The total WSS measures the compactness of the clustering and we want it to be as small as possible.

The Elbow method looks at the total WSS as a function of the number of clusters: One should choose a number of clusters so that adding another cluster doesnâ€™t improve much better the total WSS.


## LDA

```{r}
lda <- LDA(dtm, k = 9, control = list(seed = 1234))
lda
```

### Per-Topic-Per-Word Probabilities (Beta)

```{r}
# beta (per-term-per-topic) 
topics <- tidy(lda, matrix = "beta")
topics %>%
  arrange(term, -beta)  %>%
  head()
```

### Top-Level Topics

```{r fig.height=12, fig.width=12}
top_terms <- topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~ topic, scales = "free", ncol=3) +
    theme_minimal(base_size = 28) + 
    scale_y_reordered()
```

## Topic Models with Quanteda

```{r}
my_corpus <- corpus(df_select$Discussion)  # build a new corpus from the texts
summary(my_corpus[1:5])
```

```{r}
quant_dfm <- dfm(my_corpus, 
                remove_punct = TRUE, 
                remove_numbers = TRUE, 
                remove = stopwords("english"))
quant_dfm <- dfm_trim(quant_dfm, min_termfreq = 4, max_docfreq = 10)
quant_dfm
```


```{r}
set.seed(100)
if (require(stm)) {
    my_lda_fit20 <- stm(quant_dfm, K = 20, verbose = FALSE)
    plot(my_lda_fit20)    
}
```


```{r}
tmod_lda <- textmodel_lda(quant_dfm, k = 10)
```

```{r}
terms(tmod_lda, 10)
```



```{r}
dict_topic <- dictionary(file = "tesla_topics.yml")
print(dict_topic)
```

```{r}
tmod_slda <- textmodel_seededlda(quant_dfm, dictionary = dict_topic)
```

```{r}
terms(tmod_slda, 20)
```

## Targeted Dictionary Analysis

```{r}
my_corpus <- corpus(df_select$Discussion)  # build a new corpus from the texts
summary(my_corpus[1:5])
```




```{r}
quant_tesla <- select(df_select, doc_id, Discussion, User, Time)
quant_tesla
```
```{r}
quant_tesla <- quant_tesla %>%
  rename(text = Discussion)
```


```{r}
names(quant_tesla)
```


```{r}
corp_tesla <- corpus(quant_tesla)
print(corp_tesla[1:3])
```

```{r}
corp_tesla$year <- year(corp_tesla$Time)
corp_tesla$month <- month(corp_tesla$Time)
corp_tesla$week <- week(corp_tesla$Time)
```


```{r}
summary(corp_tesla)
```

```{r}
corp_tesla <- corpus_subset(corp_tesla, "year" >= 2020)
toks_tesla <- quanteda::tokens(corp_tesla, remove_punct = TRUE)
```


```{r}
lengths(data_dictionary_LSD2015)
```

```{r}
toks_tesla_lsd <- tokens_lookup(toks_tesla, dictionary =  data_dictionary_LSD2015[1:2])
head(toks_tesla_lsd, 5)
```

### Full Self Driving

```{r}
# get relevant keywords and phrases
fsd <- c("self driving", "fsd", "full self driving")

# only keep tokens specified above and their context of Â±10 tokens
toks_fsd <- tokens_keep(toks_tesla, pattern = phrase(fsd), window = 10)

toks_fsd <- tokens_lookup(toks_fsd, dictionary = data_dictionary_LSD2015[1:2])

# create a document document-feature matrix and group it by weeks in 2016
dfmat_fsd_lsd <- dfm(toks_fsd) %>% 
    dfm_group(group = "week", fill = TRUE) 

matplot(dfmat_fsd_lsd, type = "l", xaxt = "n", lty = 1, ylab = "Frequency", 
        main = "Sentiment of Self-Driving/Full Self Driving for 2020")
grid()
axis(1, seq_len(ndoc(dfmat_fsd_lsd)), ymd("2020-01-01") + weeks(seq_len(ndoc(dfmat_fsd_lsd)) - 1))
legend("topleft", col = 1:2, legend = c("Negative", "Positive"), lty = 1, bg = "white")
```

```{r}
n_fsd <- ntoken(dfm(toks_fsd, group = toks_fsd$week))
plot((dfmat_fsd_lsd[,2] - dfmat_fsd_lsd[,1]) / n_fsd, 
     type = "l", ylab = "Sentiment", xlab = "", xaxt = "n",
     main = "Sentiment of Self-Driving/Full Self Driving for 2020")
axis(1, seq_len(ndoc(dfmat_fsd_lsd)), ymd("2020-01-01") + weeks(seq_len(ndoc(dfmat_fsd_lsd)) - 1))
grid()
abline(h = 0, lty = 2)
```

```{r}
# get relevant keywords and phrases
bat <- c("battery", "charge", "range")

# only keep tokens specified above and their context of Â±10 tokens
toks_bat <- tokens_keep(toks_tesla, pattern = phrase(bat), window = 10)

toks_bat <- tokens_lookup(toks_bat, dictionary = data_dictionary_LSD2015[1:2])

# create a document document-feature matrix and group it by weeks in 2016
dfmat_bat_lsd <- dfm(toks_bat) %>% 
    dfm_group(group = "week", fill = TRUE) 

matplot(dfmat_bat_lsd, type = "l", xaxt = "n", lty = 1, ylab = "Frequency",
        main = "Sentiment of Battery/Charging/Range for 2020")
grid()
axis(1, seq_len(ndoc(dfmat_bat_lsd)), ymd("2020-01-01") + weeks(seq_len(ndoc(dfmat_bat_lsd)) - 1))
legend("topleft", col = 1:2, legend = c("Negative", "Positive"), lty = 1, bg = "white")
```

```{r}
n_bat <- ntoken(dfm(toks_bat, group = toks_bat$week))
plot((dfmat_bat_lsd[,2] - dfmat_bat_lsd[,1]) / n_bat, 
     type = "l", ylab = "Sentiment", xlab = "", xaxt = "n",
     main = "Sentiment of Battery/Charging/Range for 2020")
axis(1, seq_len(ndoc(dfmat_bat_lsd)), ymd("2020-01-01") + weeks(seq_len(ndoc(dfmat_bat_lsd)) - 1))
grid()
abline(h = 0, lty = 2)
```

```{r}
# get relevant keywords and phrases
sw <- c("software", "update")

# only keep tokens specified above and their context of Â±10 tokens
toks_sw <- tokens_keep(toks_tesla, pattern = phrase(sw), window = 10)

toks_sw <- tokens_lookup(toks_sw, dictionary = data_dictionary_LSD2015[1:2])

# create a document document-feature matrix and group it by weeks in 2016
dfmat_sw_lsd <- dfm(toks_sw) %>% 
    dfm_group(group = "week", fill = TRUE) 

matplot(dfmat_sw_lsd, type = "l", xaxt = "n", lty = 1, ylab = "Frequency",
        main = "Sentiment of Software Updates for 2020")
grid()
axis(1, seq_len(ndoc(dfmat_sw_lsd)), ymd("2020-01-01") + weeks(seq_len(ndoc(dfmat_sw_lsd)) - 1))
legend("topleft", col = 1:2, legend = c("Negative", "Positive"), lty = 1, bg = "white")
```

```{r}
n_sw <- ntoken(dfm(toks_sw, group = toks_sw$week))
plot((dfmat_sw_lsd[,2] - dfmat_sw_lsd[,1]) / n_sw, 
     type = "l", ylab = "Sentiment", xlab = "", xaxt = "n",
     main = "Sentiment of Software Updates for 2020")
axis(1, seq_len(ndoc(dfmat_sw_lsd)), ymd("2020-01-01") + weeks(seq_len(ndoc(dfmat_sw_lsd)) - 1))
grid()
abline(h = 0, lty = 2)
```

```{r}
# get relevant keywords and phrases
own <- c("take", "delivery", "purchase", "owner")

# only keep tokens specified above and their context of Â±10 tokens
toks_own <- tokens_keep(toks_tesla, pattern = phrase(own), window = 10)

toks_own <- tokens_lookup(toks_own, dictionary = data_dictionary_LSD2015[1:2])

# create a document document-feature matrix and group it by weeks in 2016
dfmat_own_lsd <- dfm(toks_own) %>% 
    dfm_group(group = "week", fill = TRUE) 

matplot(dfmat_own_lsd, type = "l", xaxt = "n", lty = 1, ylab = "Frequency",
        main = "Sentiment of Purchasing Process for 2020")
grid()
axis(1, seq_len(ndoc(dfmat_own_lsd)), ymd("2020-01-01") + weeks(seq_len(ndoc(dfmat_own_lsd)) - 1))
legend("topleft", col = 1:2, legend = c("Negative", "Positive"), lty = 1, bg = "white")
```


```{r}
n_own <- ntoken(dfm(toks_own, group = toks_own$week))
plot((dfmat_own_lsd[,2] - dfmat_own_lsd[,1]) / n_own, 
     type = "l", ylab = "Sentiment", xlab = "", xaxt = "n",
     main = "Sentiment of Purchasing Process for 2020")
axis(1, seq_len(ndoc(dfmat_own_lsd)), ymd("2020-01-01") + weeks(seq_len(ndoc(dfmat_own_lsd)) - 1))
grid()
abline(h = 0, lty = 2)
```





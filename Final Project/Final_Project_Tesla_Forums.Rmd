---
title: "Data 104 Final Project"
author: "Brian Roepkee"
date: "Dec 20, 2020"
output:
  html_document:
    df_print: paged
    code_folding: hide
    toc: true
---

# Tesla Model 3 Discussion Forum Analysis



```{r message=FALSE, warning=FALSE}
# twitter library 
library(rtweet)

# plotting and pipes
library(tidyverse)
library(ggplot2)
library(dplyr)
library(stringr)
library(tidyr)
library(ggthemes)

# text mining library
library(tm)
library(tidytext)
library(wordcloud)
library(reshape2)
library(textstem)
library(ggraph)
library(igraph)
library(widyr)
library(spacyr)
library(SnowballC)
library(topicmodels)


# date/time library
library(lubridate)
```


```{r}
df <- read.csv('tesla_forums.csv')
```

Adjust any variable formats as needed.

```{r}
df$Time <- as.Date(df$Time, "%m/%d/%y")
df$User <- as.factor(df$User)
df$Topic <- as.factor(df$Topic)
```


```{r}
str(df)
```


```{r}
summary(df)
```

```{r}
df <- drop_na(df)
```

Removed all duplicates.  The scraping method used seemed to create quite a few.

```{r}
df <- distinct(df)
```


```{r}
summary(df)
```


```{r}
df_topics <- df %>%
  group_by(Topic) %>%
  summarise(count = n(), .groups="keep") %>%
  arrange(desc(count))
head(df_topics)
```
Remove the first topic, it's just the "how to use the forums" thread and doesn't aid in analysis

```{r}
df <- df[-c(1:24), ]
```


```{r}
sprintf("There are %s unique topics", nrow(df_topics))
sprintf("There are %s total discussion threads (replies)", nrow(df))
```

### Add Doc_Id incrementing per Row

```{r}
df = df %>%
  mutate(doc_id = paste0("doc", row_number())) %>%
  select(doc_id, everything())
```

### Add a Column for Text Length

```{r}
df$text_len <- str_count(df$Discussion)
```

# EDA

Make a copy of the original DF so it can be referenced later.

```{r}
df_select <- df
```




## Tweet Length
Number of tweets by each candidate

```{r}
summary(df_select$text_len)
```

Tweets for the dataset range from `0` characters to `316` with a mean of `7470` with a mean of `284`. 


```{r warning=FALSE}
df_select %>%
  ggplot(aes(x = text_len)) +
  geom_histogram(bins = 50, fill="lightgray", color="darkgray") +
  theme_minimal() +
  scale_y_log10() +
  scale_colour_grey(start = 0.3, end = .8)
```



```{r warning=FALSE}
plot_df_time <- df %>%
  mutate(date = floor_date(Time, "week")) %>%
  group_by(date) %>%
  summarize(count = n(), .groups='keep')


ggplot(plot_df_time, aes(date, count)) +
  geom_line(show.legend = FALSE) +
  labs(x = NULL, y = "") +
  theme_minimal()
```



## Outlier Analysis

```{r}
df_select %>%
  filter(text_len > 5000) %>%
  select(Discussion) %>%
  head(n=1)
```

**Note:** Since this is discussion forum text, outliers are simply long posts as demonstrated above.  They will remain in the dataset since they are valuable information.

### Discussion Timeline

# Clean and Prepare the Text for Analysis

### Clean The Text

```{r}
df_select$Discussion <- iconv(df_select$Discussion, "latin1", "ASCII", sub = "")
df_select$Discussion <- str_replace_all(df_select$Discussion,"\\n","")
df_select$Discussion <- str_replace_all(df_select$Discussion,"@","")
df_select$Discussion <- gsub("http[[:alnum:][:punct:]]*", "", df_select$Discussion)
```


### Remove Whitespace, Punctuation, Stopwords and Lemmatize

```{r}
df_select$Discussion = removePunctuation(df_select$Discussion)
df_select$Discussion = stripWhitespace(df_select$Discussion)
df_select$Discussion = tolower(df_select$Discussion)
df_select$Discussion = removeWords(df_select$Discussion, c(stopwords('english')))
df_select$Discussion = lemmatize_strings(df_select$Discussion)
```


```{r}
head(df_select$Discussion)
```


### Unnest_Tokens()

Create a new column with each word on it's own row.

```{r}
tidy_df <- df_select %>%
  unnest_tokens(word, Discussion)
```

### Validate the New number of Rows
Dramatically larger now that each word from text is in it's own row.

```{r}
nrow(tidy_df)
```

# Sentiment Analysis

## Bing Sentiment Lexicon
Using the Bing Lexicon from Bing Liu and collaborators, adds the column "Sentiment" and mark each word as positive or negative. 

https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html

```{r}
bing_df <- tidy_df %>%
  inner_join(get_sentiments("bing"), by = "word")
```


```{r}
bing_df %>%
  group_by(sentiment) %>%
  summarise(count = n(), .groups = "keep")
```


### AFINN scoring Lexicon
AFINN from Finn Ã…rup Nielsen, adds the `value` column, with a numeric representation of how positive, or negative the word is.  The AFINN lexicon measures sentiment with a numeric score between -5 and 5

http://www2.imm.dtu.dk/pubdb/pubs/6010-full.html 

```{r}
afinn_df <- tidy_df %>%
  inner_join(get_sentiments("afinn"), by = "word")

head(afinn_df)
```


```{r}
afinn_df %>%
  ggplot(aes(x = value)) +
  geom_histogram(bins = 10, show.legend = FALSE, fill="lightgray", color="darkgray") +
  scale_x_continuous(breaks = c(-5, -3, -1, 1, 3, 5)) +
  theme_minimal() +
  scale_colour_grey(start = 0.3, end = .8)
```

For the dataset overall, there is a slight left-skew showing there is a greater concentration of words with positive values.  There are very few in the high and low values (`-4`,`-5`, `+5`). 

*Note: `0` is not a valid value in this scoring system, therefore the bin is empty*

### NRC Sentiment Lexicon
NRC from Saif Mohammad and Peter Turney. The NRC Emotion Lexicon is a list of English words and their associations with eight basic emotions as well as positive and negative sentiment.

One thing to note, single words can have multiple emotions

```{r}
nrc_df <- tidy_df %>%
  inner_join(get_sentiments("nrc"), by = "word")
```

Total counts for all 8 emotions and 2 sentiments.

```{r}
nrc_df %>%
  group_by(sentiment) %>%
  summarise(total = n(), .groups = "keep") %>%
  arrange(desc(total))
```

## Inspect Top Words Per Candidate
Using various methods, inspect what words are most frequently used, per candidate, proportions of negative and positive words, and trend over time. 

### Top Word Counts (BING)

```{r message=FALSE, warning=FALSE}
bing_df %>%
  count(word, sort = TRUE, sentiment) %>%

  group_by(sentiment) %>%
  top_n(15) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free") +
  theme_minimal() +
  labs(x = "Contribution to sentiment",
       y = NULL)
```

When looking at data as a whole, we can see the top negative word is `crisis` and positive is `win`.  Next, we'll split these out by candidate. 

### Overall Top Words (BING)

```{r message=FALSE, warning=FALSE}
bing_df %>%
  count(word, sort = TRUE, sentiment) %>%
  top_n(30) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word, fill = sentiment)) +
  geom_col(show.legend = TRUE) +
  theme_minimal() +
  labs(x = "Contribution to sentiment", y = NULL)
```


### Top Words Sorted by AFINN Score

```{r message=FALSE, warning=FALSE}
afinn_df$color <- ifelse(afinn_df$value < 0, "Negative","Positive")

afinn_df %>%
  count(word, sort = TRUE, value, color) %>%
  top_n(30) %>%
  ungroup() %>%
  mutate(word = reorder(word, value)) %>%
  ggplot(aes(value, word, fill = color)) +
  geom_col(show.legend = TRUE) +
  theme_minimal() +
  labs(x = "AFINN Sentiment Score", y = NULL)
```


### Sentiment over Time (AFINN)

```{r warning=FALSE}
plot_df2 <- afinn_df %>%
  mutate(mon = floor_date(Time, "month")) %>%
  group_by(mon) %>%
  summarize(value = mean(value), .groups = 'keep')

plot_df2$color <- ifelse(plot_df2$value < 0, "negative","positive")

ggplot(plot_df2, aes(mon, value, fill = color)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "Mean AFINN Sentiment Score") +
  theme_minimal()
```

Since August, showing the frequency of sentiment for each candidate.  Using the mean sentiment score we can see how each candidate tweeted on a daily basis.  Trumps earlier timeline tended to be more negative, while both candidates were more positive starting mid September. 


## Most Positive Messages

```{r}
afinn_df %>%
  group_by(doc_id) %>%
  summarize(total_value = sum(value), word_count = n(), .groups = "keep") %>%
  arrange(desc(total_value)) %>%
  head()
```

```{r}
df %>%
  filter(doc_id == "doc26365" | doc_id == "doc30589" | doc_id == "doc20143") %>%
  select(Discussion)
```


## Most Negative Messages

```{r}
afinn_df %>%
  group_by(doc_id) %>%
  summarize(total_value = sum(value), word_count = n(), .groups = "keep") %>%
  arrange(total_value) %>%
  head()
```

```{r}
df %>%
  filter(doc_id == "doc11357" | doc_id == "doc35860" | doc_id == "doc34072") %>%
  select(Discussion)
```

# Word Clouds

## Overall Word Cloud

```{r message=FALSE, warning=FALSE}
bing_df %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 250))
```


## By Sentiment
Word cloud of the top 200 words grouped by sentiment, positive or negative.

```{r message=FALSE, warning=FALSE}
bing_df %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("gray", "black"), max.words = 150)
```


# n-Grams

## Bi-Grams

```{r}
bigrams <- df_select %>%
  unnest_tokens(bigram, Discussion, token = "ngrams", n = 2)
```


```{r}
bigram_counts <- bigrams %>%
  count(bigram, sort = TRUE) %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigram_counts <- drop_na(bigram_counts)
bigram_counts
```


```{r message=FALSE, warning=FALSE}
bigram_graph <- bigram_counts %>%
  filter(n > 250) %>%
  graph_from_data_frame()

set.seed(2017)
a <- grid::arrow(type = "open", length = unit(.05, "inches"))

ggraph(bigram_graph, layout = "nicely") +
  geom_edge_link(arrow = a, end_cap = circle(.02, 'inches')) +
  geom_node_point(color = "gray", size = 2) +
  geom_node_text(aes(label = name), vjust = -1, hjust = 1) +
  theme_minimal()
```

## Trigrams

```{r}
trigrams <- df_select %>%
  unnest_tokens(bigram, Discussion, token = "ngrams", n = 3)
```


```{r}
trigram_counts <- trigrams %>%
  count(bigram, sort = TRUE) %>%
  separate(bigram, c("word1", "word2", "word3"), sep = " ")

trigram_counts <- drop_na(trigram_counts)
trigram_counts
```


```{r message=FALSE, warning=FALSE}
trigram_graph <- trigram_counts %>%
  filter(n > 50) %>%
  graph_from_data_frame()

set.seed(2017)
a <- grid::arrow(type = "open", length = unit(.05, "inches"))

ggraph(trigram_graph, layout = "nicely") +
  geom_edge_link(arrow = a, end_cap = circle(.02, 'inches')) +
  geom_node_point(color = "gray", size = 2) +
  geom_node_text(aes(label = name), vjust = 1.5, hjust = -.25) +
  theme_minimal()
```

# Topic Modeling

## Preprocessing

```{r}
corpus <- Corpus(VectorSource(df_select$Discussion))
```

### Before cleaning

```{r}
inspect(corpus[1:5])
```

### Remove Words that Aren't Helpful for Topic Modeling

Custom list of words generated upon performing the topic modeling.  These were frequently appeared but do not add a lot of context to topic identification, or appear so frequently, such as `University`, that they are in every topic. 

```{r}
corpus <- tm_map(corpus, removeWords, c("tesla", "get", "much", "can", 
                                        "will", "say", "car", "may", "use",
                                        "just", "one", "good", "like", "think"))
```




```{r}
inspect(corpus[1:10])
```


### Document Term Matrix Creation

```{r}
dtm <- DocumentTermMatrix(corpus)
```


```{r}
inspect(dtm)
```

### Remove Sparse Terms

```{r}
dtm = removeSparseTerms(dtm, .995)
inspect(dtm)
```



```{r}
sel_idx <- rowSums(as.matrix(dtm)) > 0
dtm <- dtm[sel_idx, ]
dim(dtm)
```

## Latent Dirichlet Allocation (LDA)
Is an unsupervised classification method designed for text data. Rather than relying on labeled (eg: hand-coded data sets), it is a probabilistic topic model that uses statistical algorithms to analyze words in raw text documents, to uncover thematic structure of the both the corpus and individual documents.

### Find the best k - the Elbow Method
The basic idea behind partitioning methods, such as k-means clustering, is to define clusters such that the total intra-cluster variation [or total within-cluster sum of square (WSS)] is minimized. The total WSS measures the compactness of the clustering and we want it to be as small as possible.

The Elbow method looks at the total WSS as a function of the number of clusters: One should choose a number of clusters so that adding another cluster doesnâ€™t improve much better the total WSS.


```{r eval=FALSE, include=FALSE}
mod_perplexity = numeric(0)
topics <- c(5:10)  

for (i in topics){
  mod <- LDA(dtm, k = i, method = "Gibbs", 
             control = list(seed=1234) ) # random seed for consitent results between runs
  mod_perplexity[i] = perplexity(mod, dtm)
}
mod_perplexity <- mod_perplexity[!is.na (mod_perplexity)] # remove NA at first position

# plot
plot(x=topics, y=mod_perplexity, type = "b", xlab = "Number of topics", ylab = "Perplexity")
```


https://www.datanovia.com/en/lessons/determining-the-optimal-number-of-clusters-3-must-know-methods/ 

```{r eval=FALSE, include=FALSE}
mat <- as.matrix(weightTfIdf(dtm))

# normalize the TfIdf scores by euclidean distance. 
scaled_data  <- dist(mat, method = "euclidean")

k.max <- 10
data <- scaled_data
wss <- sapply(1:k.max, 
  function(k){kmeans(data, k, nstart=50,iter.max = 15)$tot.withinss})

plot(1:k.max, wss,
     type="b", pch = 19, frame = FALSE, 
     xlab="Number of clusters K",
     ylab="Total within-clusters sum of squares")
```

```{r eval=FALSE, include=FALSE}
library(parallel)
library( ldatuning )

sequ <- seq(5, 10, 1) 

no_cores <- detectCores() - 1 #Calculate the number of cores

#Run LDA using Gibbs sampling
result <- FindTopicsNumber(
  dtm,
  topics = sequ,
  metrics = c("CaoJuan2009", "Deveaud2014", "Griffiths2004", "Arun2010"),
  method = "Gibbs",
  control = list(
    seed = 123
  ),
  mc.cores = no_cores
)

FindTopicsNumber_plot(result)
```


## LDA

```{r}
lda <- LDA(dtm, k = 9, control = list(seed = 1234))
lda
```

### Per-Topic-Per-Word Probabilities (Beta)

```{r}
# beta (per-term-per-topic) 
topics <- tidy(lda, matrix = "beta")
topics %>%
  arrange(term, -beta)  %>%
  head()
```

### Top-Level Topics

```{r fig.height=12, fig.width=12}
top_terms <- topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~ topic, scales = "free", ncol=3) +
    theme_minimal(base_size = 28) + 
    scale_y_reordered()
```




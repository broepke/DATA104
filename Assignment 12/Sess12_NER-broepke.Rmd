---
title: "Assignment 12"
author: "Your Name"
date: "Nov 30, 2020"
output:
  html_document:
    df_print: paged
---
# NER and Topic Modeling


```{r message=FALSE, warning=FALSE}
# Basic libraries
library(ggplot2)
library(dplyr)
library(tidyverse)


# Text Mining and NLP Libraries
library(spacyr)
library(tm)
library(tidytext)
library(SnowballC)
library(stringr)
library(topicmodels)
library(textstem)

# date/time library
library(lubridate)
```


```{r}
spacy_initialize(model = "en_core_web_sm", condaenv="DATA104")
```

## Data Loading

```{r}
df <- read.csv("news_groups.csv")
```

## Transformations

 * Add text_len to capture the length of each document (eg: text field)
 * Add doc_id with values 'doc_id'<row id>. Example: row 1, 2 should have doc_id = doc1 , doc2, doc3 ....  docN.


```{r}
# convert dataframe to tibble
tidy_df <- as_tibble(df$content)

# the text is itself a dataframe? modify the text column to pull out the text value
tidy_df <- tidy_df %>%
  mutate(text =  tidy_df$value)  %>%
  select(-value)
```


```{r}
tidy_df = tidy_df %>%
  mutate(text_len = str_count(text))

head(tidy_df)
```


```{r}
# convert to TIF standard required by spacy
tidy_df = tidy_df %>%
  mutate(doc_id = paste0("doc", row_number())) %>%
  select(doc_id, everything())
head(tidy_df)
```

## Summary Stats and Structure

Output summary statics and discuss the shape of the dataset along with the text_len statistics and outliers.

```{r}
str(tidy_df)
```

```{r}
tidy_df %>%
  select(text, text_len) %>%
  summary()
```

 * **text**: Length represents the number of observations, `1,794` in this dataset. 
 * **text_len**: The character count of the entries for each observation.  A min of 171 with a max of 36,272.  Using the 3rd Quartile, most of the values in this fall under 2,000 characters.

```{r}
head(tidy_df)
```

## Outliers Analysis and Removal

visualization: show histogram and boxplot to better visualize outliers.  

Create an outliers data frame. Refer to: https://www.statsandr.com/blog/outliers-detection-in-r/#histogram
Decide which outliers (if any) should be removed from your analysis, by inspecting some of the contents.  You can use the c() function to display the entire contents of the text.

```{r}
summary(tidy_df$text_len)
```

### Text Length Distribution
Log 10 adjusted scale

```{r}
ggplot(tidy_df) +
  aes(x = text_len) +
  geom_histogram(bins = 30L, fill = "lightblue", color = "lightblue4") +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold")) +
  labs(
    x = "Text Length", y = "Count",
    title = "Distribution of Text Length",
    subtitle = "Without any Modifications to the Datset"
  )
```
The data is highly right-skewed, with most of the values around the median of `1,201` or so words.  We can see that the long tail towards the larger values with a max of `36,272`.

### Text Length Box Plot

```{r}
ggplot(tidy_df) +
  aes(x = "", y = text_len) +
  geom_boxplot(fill = "lightblue") +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold")) +
  labs(
    x = NULL, y = "Text Length",
    title = "Distribution of Text Length",
    subtitle = "Without any Modifications to the Datset"
  )
```

The box plot shows us a similar view, with a number of outliers beyond the inner-quartile. and a high concentraion around `~1,500`.  

#### Determine Outliers
We can determine which rows represent outliers and remove those from our dataset. 

```{r}
out <- boxplot.stats(tidy_df$text_len)$out
out_ind <- which(tidy_df$text_len %in% c(out))
out_ind
```

### Remove all outliers 

```{r}
sprintf("Original Rows = %s", nrow(tidy_df))
sprintf("Number of Outliers = %s", length(out_ind))
tidy_df <- tidy_df[-out_ind, ]
sprintf("New Number of Rows = %s", nrow(tidy_df))
```

After removing the outliers from the dataset, we're left with 1,662 total observations.  We can examine the summary stats and box plot again.

```{r}
summary(tidy_df$text_len)
```

```{r}
ggplot(tidy_df) +
  aes(x = text_len) +
  geom_histogram(bins = 30L, fill = "lightblue", color="lightblue4") +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold")) +
  labs(
    x = "Text Length", y = "Count",
    title = "Distribution of Text Length",
    subtitle = "After Outliers Removed"
  )
```

After removing outliers, the shape of the histogram is still right-skewed, but not as dramatic as before.

```{r}
ggplot(tidy_df) +
  aes(x = "", y = text_len) +
  geom_boxplot(fill = "lightblue") +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold")) +
  labs(
    x = NULL, y = "Text Length",
    title = "Box Plot of Text Length",
    subtitle = "After Outliers Removed"
  )
```

The boxplot also shows a much less compressed view of the inner-quartile range of data.  There are still some outliers plotted but extreme values have been removed. 

# NER

Conduct Named entity recognition.  Do not perform pre-processing prior. Algorithms and tools such as spaCy often rely on semantic language structures in order to identify entities (eg: punctuation, capitalization for pronouns, etc.) Generally NER requires tokenization and pos tagging, however the spacyR library automatically performs this for you.

## Sample the Data to a Smaller Subset.
Using a random see (1234), to provide consistent results between runs.

```{r}
# sample smaller set of docs
set.seed(1234)
sample_df = tidy_df %>%
  sample_n(1000) 
```

## Add Named Entity Types
Using the spacyR library.

```{r}
# parse name-entities
parsedtxt <- spacy_parse(sample_df, type = "all")

ner = entity_extract(parsedtxt)  %>%
  arrange(entity_type)
head(ner)
```

### Part of Speech Tagging Counts

```{r}
# Parts of Speech Tagging Coutns
parsedtxt %>%
  group_by(pos) %>%
  summarise(count = n(), .groups = "keep") %>%
  arrange(desc(count))
```

### Entity Type Counts

```{r}
ner %>%
  group_by(entity_type) %>%
  summarise(count = n(), .groups = "keep") %>%
  arrange(desc(count))
```

### Entity Types
https://spacy.io/api/annotation

 * Entities of type PERSON which are People, including fictional.
 * Entities of type NORP which are Nationalities or religious or political groups.
 * Entities of type ORG which are Companies, agencies, institutions, etc.
 * Entities of type GPE which are geo-political entities such as city, state/province, and country
 * Entities of type PRODUCT which are products
 * Entities of type WORK OF ART which are Titles of books, songs, etc.
 * Entities of type LOC which are Non-GPE locations, mountain ranges, bodies of water.
 * Entities of type FAC which are Buildings, airports, highways, bridges, etc.
 * Entities of type EVENT which are Named hurricanes, battles, wars, sports events, etc.
 * Entities of type LAW which are Named documents made into laws.
 * Entities of type LANGUAGE which are Any named language.
 

```{r}
entities<- c("PERSON", "NORP", "ORG", "GPE", "PRODUCT", "WORK",
             "LOC", "FAC", "EVENT", "LAW", "LANGUAGE")

for (e in entities){
  ner %>%
    mutate(type=e) %>%
    filter(entity_type == e) %>%
    group_by(entity, type) %>%
    summarize(count = n(), .groups = "keep") %>%
    arrange(desc(count)) %>%
    print()
  
}
```

# Topic Modeling

Conduct Topic Modeling using LDA , referring to the reading and sample notebook for guidance.

perform standard text preprocessing as well as some additions. Research regular expressions to help you.
case normalization (eg: either lower-case or upper-case)
 remove emails, stop words, punctuation, numbers,  new lines, single quotes
lemmatization
include visualizations of topics and word groupings
Assess the model. 
Do the topics and words make sense given visual inspection? 

## Preprocessing

```{r}
corpus <- Corpus(VectorSource(sample_df$text))
```


```{r}
inspect(corpus[1:5])
```


```{r}
corpus <- tm_map(
  corpus,
  content_transformer(
    function(x) str_replace_all(x,"[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+", "")
    )
)
```

```{r}
corpus <- tm_map(
  corpus,
  content_transformer(
    function(x) gsub("http[[:alnum:][:punct:]]*", "", x)
    )
)
```

```{r}
# remove new lines and tabs
corpus <- tm_map(
  corpus,
  content_transformer(
    function(x) gsub("\\n", " ", x)
    )
)

corpus <- tm_map(
  corpus,
  content_transformer(
    function(x) gsub("\\t", " ", x)
    )
)
```

```{r}
# force all to lowercase
corpus <- tm_map(corpus, tolower)
```


```{r}
# get rid of the useless newsgroup stuff
corpus <- tm_map(
  corpus,
  content_transformer(
    function(x) gsub("from:", " ", x)
    )
)

corpus <- tm_map(
  corpus,
  content_transformer(
    function(x) gsub("subject:", " ", x)
    )
)

corpus <- tm_map(
  corpus,
  content_transformer(
    function(x) gsub("re:", " ", x)
    )
)
```


```{r}
corpus <- tm_map(corpus, removePunctuation)  # remove punctuation
corpus <- tm_map(corpus, removeNumbers)      # remove numbers
corpus <- tm_map(corpus, stripWhitespace)    # remove white space
```


```{r}
# stop and custom stop 
corpus <- tm_map(corpus, removeWords, c(stopwords('english')))
```


```{r}
corpus <- tm_map(corpus, lemmatize_strings) # lemmatizaton
```

```{r}
# remove words from the corpus that aren't providing extra clarity, but are
# appearing frequently. These are updated after multiple LDA analyses.
corpus <- tm_map(corpus, removeWords, c("line", "write", 
                                        "article", "get", "good", "one", 
                                        "year", "nntppostinghost", "can", 
                                        "say", "will", "know", "may", 
                                        "like", "see", "time", "just", 
                                        "make", "think", "thing", "way", 
                                        "come", "much", "want", "take",
                                        "ive", "day", "first", "xnewsreader",
                                        "university", "also", "people", "use",
                                        "organization", "dod", "distribution"))
```


```{r}
inspect(corpus[1:5])
```

### Lemmatization

```{r}
dtm <- DocumentTermMatrix(corpus, control = list(minWordLength = 5, 
                removeNumbers=FALSE, removePunctuation=FALSE, 
                removeStopwords=FALSE,  stemWords=FALSE, stripWhitespace=FALSE))
```


```{r}
inspect(dtm)
```

```{r}
# check for rows with all 0's
zero_row <- sample_df[rowSums(as.matrix(dtm)) == 0, ] 
dim(zero_row)
```

```{r}
sel_idx <- rowSums(as.matrix(dtm)) > 0
dtm <- dtm[sel_idx, ]
dim(dtm)
```

## Latent Dirichlet Allocation (LDA)
Is an unsupervised classification method designed for text data. Rather than relying on labeled (eg: hand-coded data sets), it is a probabilistic topic model that uses statistical algorithms to analyze words in raw text documents, to uncover thematic structure of the both the corpus and individual documents. To use it requires: - A document-term matrix - The number of topics you would like the algorithm to pick up.

### Find the best k
Recall that LDA requires that we supply the number of topics (K). If you have prior knowledge on the topic structure, this can be used to inform your selection of a reasonable value for K. Too few could result in groupings of unrelated content while too large could risk being overly granular with the topic categories. There are several statistical measures that can help in finding an optimal value for K.
Perplexity is one and is used to measure how well a probability model predicts a sample. As applied to LDA, for a given value of K, one can estimate the LDA model by comparing perplexity across different models with varying kâ€™s. The model with the lowest perplexity is generally considered the best.

https://en.wikipedia.org/wiki/Perplexity 

```{r}
mod_perplexity = numeric(0)
topics <- c(36:45)  

for (i in topics){
  mod <- LDA(dtm, k = i, method = "Gibbs", 
             control = list(seed=1234) )
  mod_perplexity[i] = perplexity(mod, dtm)
}
mod_perplexity <- mod_perplexity[!is.na (mod_perplexity)] # remove NA at first position

# plot
plot(x=topics, y=mod_perplexity, type = "b", xlab = "Number of topics", ylab = "Perplexity")
```

After calculating Perplexity for the range of topics from 2-45 and continued to find generally decreasing values (depending on the run).  However, this would most likely be at a level of granularity greater than we need, so we can pick a smaller number to start and test the results.

### Per-term-per-topic matrix

```{r}
hl_lda <- LDA(dtm, k = 12, control = list(seed = 1234))
hl_lda
```


```{r}
# beta (per-term-per-topic) 
hl_topics <- tidy(hl_lda, matrix = "beta")
hl_topics %>%
  arrange(term, -beta)  %>%
  head()
```

### per-document-per-topic

```{r}
tidy(hl_lda, matrix = "gamma")  %>%
  arrange(document, -gamma)  %>%
  head()
```

### Show the 10 terms that are most common within each topic.

```{r fig.height=24, fig.width=12}
hl_top_terms <- hl_topics %>%
  group_by(topic) %>%
  top_n(15, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

hl_top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~ topic, scales = "free", ncol=2) +
    theme_classic(base_size = 20) + 
    scale_y_reordered() 
```

Based on 12 topics, we can see there is a lot of overlap with the top words used in various topics. 

### High Level Groupings


```{r}
hl_lda2 <- LDA(dtm, k = 3, control = list(seed = 1234))
hl_lda2
```


```{r}
# beta (per-term-per-topic) 
hl_topics2 <- tidy(hl_lda2, matrix = "beta")
hl_topics2 %>%
  arrange(term, -beta)  %>%
  head()
```
### Top-Level Topics

```{r fig.height=5, fig.width=12}
hl_top_terms2 <- hl_topics2 %>%
  group_by(topic) %>%
  top_n(15, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

hl_top_terms2 %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~ topic, scales = "free", ncol=3) +
    theme_classic(base_size = 20) + 
    scale_y_reordered() 
```

After reviewing the text from the 12 topics above, it appears that this data comes from three distinct discussion groups.

 1. **Baseball** - game, run, score, team, hit, base, pitch, win, and ball are all solid indications that this is true.
 2. **Religion** - god, christian, believe, jesus, church, bible, faith, sin are all words directed tied to religion and possible Christianity.
 3. **Motorcycles** - bike, ride, motorcycle are all potentenial for the third grouping.  But it's possible that there are only two main groupings, and this is a sub grouping. 


---
title: "Data Camp WebScraping"
---

```{r}
library(rvest)
library(tidyverse)
library(httr)
```

# Intro to HTML

```{r}
html_excerpt_raw <- '
<html> 
  <body> 
    <h1>Web scraping is cool</h1>
    <p>It involves writing code – be it R or Python.</p>
    <p><a href="https://datacamp.com">DataCamp</a> 
		has courses on it.</p>
  </body> 
</html>'
# Turn the raw excerpt into an HTML document R understands
html_excerpt <- read_html(html_excerpt_raw)
html_excerpt
# Print the HTML excerpt with the xml_structure() function
xml_structure(html_excerpt)
```



```{r}
list_html_raw <- "\n<html>\n
<body>\n
<ol>\n
<li>Learn HTML</li>\n
<li>Learn CSS</li>\n
<li>Learn R</li>\n
<li>Scrape everything!*</li>\n
</ol>\n
<small>*Do it responsibly!</small>\n
</body>\n
</html>"

# Read in the corresponding HTML string
list_html <- read_html(list_html_raw)
# Extract the ol node
ol_node <- list_html %>% html_node('ol')
# Extract and print the nodeset of all the children of ol_node
html_children(ol_node)
```


```{r}
hyperlink_html <-  "
\n<html>\n  
<body>\n    
<h3>Helpful links</h3>\n    
<ul>\n      
<li><a href=\"https://wikipedia.org\">Wikipedia</a></li>\n
<li><a href=\"https://dictionary.com\">Dictionary</a></li>\n 
<li><a href=\"https://duckduckgo.com\">Search Engine</a></li>\n
</ul>\n    
<small>\n 
Compiled with help from <a href=\"https://google.com\">Google</a>.\n
</small>\n  
</body>\n
</html>"

# Extract all the a nodes from the bulleted list
links <- hyperlink_html %>% 
  read_html() %>%
  html_nodes('li a')

# Parse the nodes into a data frame
link_df <- tibble(
  domain = links %>% html_attr('href'),
  name = links %>% html_text()
)

link_df
```



# Intro to CSS

```{r}
languages_raw_html <- "\n<html> \n  <body> \n    <div>Python is perfect for programming.</div>\n    <p>Still, R might be better suited for data analysis.</p>\n    <small>(And has prettier charts, too.)</small>\n  </body> \n</html>"
# Read in the HTML
languages_html <- read_html(languages_raw_html)
# Select the div and p tags and print their text
languages_html %>%
	html_nodes('div, p') %>%
	html_text()
```

```{r}
structured_raw_html <- "<html>
  <body>
    <div id = 'first'>
      <h1 class = 'big'>Joe Biden</h1>
      <p class = 'first blue'>Democrat</p>
      <p class = 'second blue'>Male</p>
    </div>
    <div id = 'second'>...</div>
    <div id = 'third'>
      <h1 class = 'big'>Donald Trump</h1>
      <p class = 'first red'>Republican</p>
      <p class = 'second red'>Male</p>
    </div>
  </body>
</html>"
structured_html <- read_html(structured_raw_html)
structured_html %>%
  html_nodes('#first')
```

```{r}
nested_html <- read_html("<html>
  <body>
    <div>
      <p class = 'text'>A sophisticated text [...]</p>
      <p class = 'text'>Another paragraph following [...]</p>
      <p class = 'text'>Author: T.G.</p>
    </div>
    <p>Copyright: DC</p>
  </body>
</html>")

nested_html  %>% 
	html_nodes('p.text:last-child')
```


```{r}
languages_html <- read_html("  <ul id = 'languages'>
    <li>SQL</li>
    <ul>    
      <li>Databases</li>
      <li>Query Language</li>
    </ul>
    <li>R</li>
    <ul>
      <li>Collection</li>
      <li>Analysis</li>
      <li>Visualization</li>
    </ul>
    <li>Python</li>
  </ul>")

# Extract only the text of the computer languages (without the sub lists)
languages_html %>% 
	html_nodes('ul#languages > li') %>% 
	html_text
```


```{r}
complicated_html <- read_html('<html>
  <body>
    <div class="first section">
      A text with a <a href="#">link</a>.
    </div>
    <div class="second section">
      Some text with <a href="#">another link</a>.
      <div class="first paragraph">Some text.</div>
      <div class="second paragraph">Some more text.
      <div>...</div>
    </div>
  </div>
</body>
</html>')

# Select the three divs with a simple selector
complicated_html %>%
	html_nodes('div div')
```


```{r}
code_html <- read_html("<html> 
<body> 
  <h2 class = 'first'>First example:</h2>
  <code>some = code(2)</code>
  <span>will compile to...</span>
  <code>some = more_code()</code>
  <h2 class = 'second'>Second example:</h2>
  <code>another = code(3)</code>
  <span>will compile to...</span>
  <code>another = more_code()</code>
</body> 
</html>")

# Select the first code elements in the second example
code_html %>% 
	html_nodes('h2.second + code')

# Select all code elements in the second example
code_html %>% 
	html_nodes('h2.second ~ code')
  
```


# Intro to XPATH

```{r}
weather_html <- read_html("<html>
  <body>
    <div id = 'first'>
      <h1 class = 'big'>Berlin Weather Station</h1>
      <p class = 'first'>Temperature: 20°C</p>
      <p class = 'second'>Humidity: 45%</p>
    </div>
    <div id = 'second'>...</div>
    <div id = 'third'>
      <p class = 'first'>Sunshine: 5hrs</p>
      <p class = 'second'>Precipitation: 0mm</p>
    </div>
  </body>
</html>")

print("Select all p elements")
weather_html %>%
	html_nodes(xpath = '//p')


print("Select p elements with the second class")
weather_html %>%
	html_nodes(xpath = '//p[@class = "second"]')

print("Select p elements that are children of #third")
weather_html %>%
	html_nodes(xpath = "//div[@id = 'third']/p")

print("Select p elements with class second that are children of #third")
weather_html %>%
	html_nodes(xpath = "//div[@id = 'third']/p[@class = 'second']")
```


```{r}
weather_html <- read_html("<html>
  <body>
    <div id = 'first'>
      <h1 class = 'big'>Berlin Weather Station</h1>
      <p class = 'first'>Temperature: 20°C</p>
      <p class = 'second'>Humidity: 45%</p>
    </div>
    <div id = 'second'>...</div>
    <div id = 'third'>
      <p class = 'first'>Sunshine: 5hrs</p>
      <p class = 'second'>Precipitation: 0mm</p>
      <p class = 'third'>Snowfall: 0mm</p>
    </div>
  </body>
</html>")

# Select all divs
weather_html %>% 
  html_nodes(xpath = '//div')

# Select all divs with p descendants
weather_html %>% 
  html_nodes(xpath = '//div[p]')

# Select all divs with p descendants having the "third" class
weather_html %>% 
  html_nodes(xpath = '//div[p[@class = "third"]]')
```

# Advanced XPATH with Predicates

```{r}
rules_html <- read_html("<div>
  <h2>Today's rules</h2>
  <p>Wear a mask</p>
  <p>Wash your hands</p>
</div>
<div>
  <h2>Tomorrow's rules</h2>
  <p>Wear a mask</p>
  <p>Wash your hands</p>
  <p>Bring hand sanitizer with you</p>
</div>")

# Select the text of the second p in every div
rules_html %>% 
  html_nodes(xpath = "//div/p[position() = 2]") %>%
  html_text

# Select every p except the second from every div
rules_html %>% 
  html_nodes(xpath = "//div/p[position() != 2]") %>%
  html_text

# Select the text of the last three nodes of the second div
rules_html %>% 
  html_nodes(xpath = '//div[position() = 2]/*[position() >= 2]') %>%
  html_text()
```

```{r}
forecast_html <- read_html("<div>
  <h1>Tomorrow</h1>
</div>
<div>
  <h2>Berlin</h2>
  <p>Temperature: 20°C</p>
  <p>Humidity: 50%</p>
</div>
<div>
  <h2>London</h2>
  <p>Temperature: 15°C</p>
</div>
<div>
  <h2>Zurich</h2>
  <p>Temperature: 22°C</p>
  <p>Humidity: 60%</p>
</div>")

# Select only divs with one header and at least one paragraph
forecast_html %>%
	html_nodes(xpath = '//div[count(h2) = 1 and count(p) > 1]')
```

```{r}
roles_html <- read_html('<table>
 <tr>
  <th>Actor</th>
  <th>Role</th>
 </tr>
 <tr>
  <td class = "actor">Jayden Carpenter</td>
  <td class = "role"><em>Mickey Mouse</em> (Voice)</td>
 </tr>
 ...
</table>')

# Extract the data frame from the table using a known function from rvest
roles <- roles_html %>% 
  html_node(xpath = "//table") %>% 
  html_table()
# Print the contents of the role data frame
print(roles)

# Extract the actors in the cells having class "actor"
actors <- roles_html %>% 
  html_nodes(xpath = '//table//td[@class = "actor"]') %>%
  html_text()
actors

# Extract the roles in the cells having class "role"
roles <- roles_html %>% 
  html_nodes(xpath = '//table//td[@class = "role"]/em') %>% 
  html_text()
roles

# Extract the functions using the appropriate XPATH function
functions <- roles_html %>% 
  html_nodes(xpath = '//table//td[@class = "role"]/text()') %>%
  html_text(trim = TRUE)
functions

# Create a new data frame from the extracted vectors
cast <- tibble(
  Actor = actors, 
  Role = roles, 
  Function = functions)

cast
```

# The nature of HTTP requests

```{r}
# Get the HTML document from Wikipedia using httr
wikipedia_response <- GET('https://en.wikipedia.org/wiki/Varigotti')
# Check the status code of the response
status_code(wikipedia_response)
# Parse the response into an HTML doc
wikipedia_page <- read_html(wikipedia_response)
# Extract the altitude with XPATH
wikipedia_page %>% 
	html_nodes(xpath = '//table//tr[position() = 9]/td') %>% 
	html_text()
```

```{r}
# Access https://httpbin.org/headers with httr
response <- GET("https://httpbin.org/headers")
# Print its content
content(response)
```

# Throttleing Calls

```{r}
throttled_read_html <- slowly(~ read_html("https://wikipedia.org"),
                    rate = rate_delay(0.5))

for(i in c(1, 2, 3)){
  throttled_read_html("https://google.com") %>% 
      html_node("title") %>% 
      html_text() %>%
    print()
}      
```

```{r}
a <- "https://en.wikipedia.org/w/index.php?title=Mount_Everest&oldid=958643874"
b <- "https://en.wikipedia.org/w/index.php?title=K2&oldid=956671989"           
c <- "https://en.wikipedia.org/w/index.php?title=Kangchenjunga&oldid=957008408"

mountain_wiki_pages <- c(a,b,c)

# Define a throttled read_html() function with a delay of 0.5s
read_html_delayed <- slowly(read_html, 
                            rate = rate_delay(0.5))
# Construct a loop that goes over all page urls
for(page_url in mountain_wiki_pages){
  # Read in the html of each URL with a delay of 0.5s
  html <- read_html_delayed(page_url)
  # Extract the name of the peak and its coordinates
  peak <- html %>% 
  	html_node("#firstHeading") %>% html_text()
  coords <- html %>% 
    html_node("#coordinates .geo-dms") %>% html_text()
  print(paste(peak, coords, sep = ": "))
}
```


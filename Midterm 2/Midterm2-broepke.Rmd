---
title: "Midterm 2"
author: "Brian Roepke"
date: "November 22, 2020"
output:
  html_document:
    df_print: paged
---
# YouTube Video Data Analysis

```{r message=FALSE, warning=FALSE}
# Basic libraries 
library(ggplot2)
library(dplyr)
library(tidyverse)
library(corrplot)
library(ggcorrplot)
library(lubridate)
library(jsonlite)
library(stringi)

# Text mining libs
library(SnowballC)
library(tidytext)
library(spacyr)
library(tm)
library(wordcloud2)
library(ggraph)
library(textstem)
library(ggridges)
```


```{r}
# Read in the Data from a CSV file.
df <- read.csv('YouTube-videos.csv') #colClasses=c("headline"="character")
```


```{r}
# Remove unwanted columns
df$thumbnail_link <- NULL
df$comments_disabled <- NULL
df$video_error_or_removed <- NULL
df$ratings_disabled <- NULL

# Convert Dates
df$trending_date <- as.Date(df$trending_date, "%y.%d.%m")
df$publish_time <- as.Date(df$publish_time, "%Y-%m-%d")

# Change Others to Factors
df$category_id <- as.factor(df$category_id)
```


```{r}
# Import the YouTube Category Names
cats <- fromJSON("youtubeVideoCatUS.json", flatten = TRUE)
cats <- as.data.frame(cats)
```


```{r}
# Create a new column that contains the English name of the category based on the Category ID
df$category_name <- cats$items.snippet.title[match(df$category_id, cats$items.id)]
df$category_name <- as.factor(df$category_name)
```

## Cleaning
Cleaning up the NAME? in video_id

```{r}
df %>%
  filter(video_id == '#NAME?') %>%
  summarize(total_records = n())
```

Replace each occurrence of #NAME? with a random generated string.

```{r}
df$video_id[df$video_id == '#NAME?'] <- stri_rand_strings(1, 11)
df$video_id <- as.factor(df$video_id)
```


# EDA
Perform Exploratory Data Analysis to better understand the data. 

## Summary Statistics

```{r}
str(df)
```


```{r}
df %>%
  select(video_id, trending_date, publish_time, views, likes, dislikes, comment_count, category_name) %>%
  summary()
```

 * **NOTE**: Video_ID for the first user shows the same count of `525` as the prior "$NAME?" value.

```{r}
head(df)
tail(df)
```

## Views, Likes and Dislikes
 * Analyze the number of views, likes, and dislikes with full data visualization (univariate and multivariate). Then comment on various trends.
 
### Views
 
```{r}
df %>%
  ggplot(aes(x = views)) +
  geom_histogram(color = "lightblue3", fill = "lightblue", bins = 30) + 
  scale_x_continuous(trans='log10') +
  theme_minimal() +
  theme(
  plot.title = element_text(face = "bold")) +
  labs(
  x = NULL, y = "Count (Log 10)",
  title = "Distribution of Views",
  subtitle = "Using a log10 transformation the x-axis"
  )
```


```{r}
df %>%
  group_by(category_name) %>%
  summarise(total_views = mean(views), .groups = "keep") %>%
  arrange(desc(total_views)) %>%
  ggplot(aes(x= category_name, y = total_views)) +
  geom_col(color = "lightblue3", fill = "lightblue") + 
  theme_minimal() +
  theme(
  plot.title = element_text(face = "bold")) +
  labs(
  x = NULL, y = "Count",
  title = "Mean Number of Videos Views per Category"
  ) +
  coord_flip()
```

```{r warning=FALSE}
df %>%
  ggplot(aes(x=views, color=category_name, fill=category_name)) +
    geom_histogram(alpha=0.6, bins = 30) +
    scale_x_continuous(trans='log10') +
    theme_minimal() +
    theme(
      legend.position="none",
      panel.spacing = unit(0.1, "lines"),
      strip.text.x = element_text(size = 8)
    ) +
    xlab("") +
    ylab("") +
    facet_wrap(~category_name)
```

```{r}
df %>%
    ggplot(aes(views, color = category_name)) +
    geom_boxplot() +
    scale_x_continuous(trans='log10') +
    labs(title = "Video Views Distribution by Category",
         x = "Views (Log10 Transformation)",
         y = NULL) +
    theme_minimal() +
    theme(
      legend.title = element_blank(),
      legend.position = "right",
      plot.title = element_text(face = "bold"))
```


```{r}
df %>%
  mutate(mon = floor_date(trending_date, 'month')) %>%
  group_by(mon, category_name) %>%
  summarize(total = mean(views), .groups = 'keep') %>%

  ggplot(aes(x=mon, y=total, fill=category_name)) +
  geom_col(color = "black") + 
  scale_x_date(date_breaks = "1 month", expand = c(0,0), date_labels = "%b-%y") +
  theme_classic() +
  theme(
  plot.title = element_text(face = "bold")) +
  labs(
  x = "Trending Date", y = "Views",
  title = "Mean Number of Views per Category",
  subtitle = "Grouped by Month for Trending Date"
  )
```

 
### Likes

```{r warning=FALSE}
df %>%
  ggplot(aes(x = likes)) +
  geom_histogram(color = "gray", fill = "lightgray", bins = 30) + 
  scale_x_continuous(trans='log10') +
  theme_minimal() +
  theme(
  plot.title = element_text(face = "bold")) +
  labs(
  x = NULL, y = "Count (Log 10)",
  title = "Distribution of Likes",
  subtitle = "Using a log10 transformation the x-axis"
  )
```



```{r warning=FALSE}
df %>%
  ggplot(aes(x=likes, color=category_name, fill=category_name)) +
    geom_histogram(alpha=0.6, bins = 30) +
    scale_x_continuous(trans='log10') +
    theme_minimal() +
    theme(
      legend.position="none",
      panel.spacing = unit(0.1, "lines"),
      strip.text.x = element_text(size = 8)
    ) +
    xlab("") +
    ylab("") +
    facet_wrap(~category_name)
```


```{r}
df %>%
  mutate(mon = floor_date(trending_date, 'month')) %>%
  group_by(mon, category_name) %>%
  summarize(total = mean(likes), .groups = 'keep') %>%

  ggplot(aes(x=mon, y=total, fill=category_name)) +
  geom_col(color = "black") + 
  scale_x_date(date_breaks = "1 month", expand = c(0,0), date_labels = "%b-%y") +
  theme_classic() +
  theme(
  plot.title = element_text(face = "bold")) +
  labs(
  x = "Trending Date", y = "Likes",
  title = "Number of Likes per Category",
  subtitle = "Grouped by Month for Trending Date"
  )
```

### Dislikes

```{r warning=FALSE}
df %>%
  ggplot(aes(x = dislikes)) +
  geom_histogram(color = "gray", fill = "lightgray", bins = 30) + 
  scale_x_continuous(trans='log10') +
  theme_minimal() +
  theme(
  plot.title = element_text(face = "bold")) +
  labs(
  x = NULL, y = "Count (Log 10)",
  title = "Distribution of Dislikes",
  subtitle = "Using a log10 transformation the x-axis"
  )
```



```{r}
df %>%
  mutate(mon = floor_date(trending_date, 'month')) %>%
  group_by(mon, category_name) %>%
  summarize(total = mean(dislikes), .groups = 'keep') %>%

  ggplot(aes(x=mon, y=total, fill=category_name)) +
  geom_col(color = "black") + 
  scale_x_date(date_breaks = "1 month", expand = c(0,0), date_labels = "%b-%y") +
  theme_classic() +
  theme(
  plot.title = element_text(face = "bold")) +
  labs(
  x = "Trending Date", y = "Dislikes",
  title = "Number of Dislikes per Category",
  subtitle = "Grouped by Month for Trending Date"
  )
```

```{r}
df %>%
  filter(trending_date >= "01-01-2018" & trending_date > "02-01-2018" & category_name == "People & Blogs") %>%
  arrange(desc(dislikes))
```


## Top 5 Videos
 * Identify the top 5 videos based on the number of views and number of likes
 
## Top 5 Videos based on Likes

The following are the top five videos based on **Likes**.  We can see that the top 5 videos are all the same, but with different trending dates. 
 
```{r}
df %>%
  arrange(desc(likes)) %>%
  select(trending_date, title, likes) %>%
  head(n=5)
```
 
The following are the top five unique videos based on **likes**.  
 
```{r}
df %>%
  arrange(desc(likes)) %>%
  distinct(video_id, .keep_all = TRUE) %>%
  select(trending_date, title, likes) %>%
  head(n=5) 
```
 
### Top 5 Videos based on Views

The following are the top five unique videos based on **views**. 

```{r}
df %>%
  arrange(desc(views)) %>%
  distinct(video_id, .keep_all = TRUE) %>%
  select(trending_date, title, views) %>%
  head(n=5) 
```
 
 
## Correlation to Likes
 * What numeric values correlate to likes?
 
```{r}
df_corr = df %>% 
  select_if(is.numeric) %>%
  # reordering the numeric columns so likes is listed first. 
  select(likes, views, dislikes, comment_count)
```

## Correlation Test

In order to find out which numeric values correlate to likes, we can create a correlation matrix and correlation plot. 

```{r}
corr <- round(cor(df_corr), 2)
corr
```

We can see that all of the other numeric values positively correlate to likes with comment_count being the strongest at `0.84`, views being the next strongest at `0.83`, dislikes at `0.46`

Next we can see a visualization of these values. 

```{r}
ggcorrplot(corr, colors = c("#6D9EC1", "white", "#E46726"),lab = TRUE,  ggtheme = ggplot2::theme_gray)
```
### Scatterplot 

```{r warning=FALSE}
df %>%
  ggplot(aes(x = likes, y = comment_count)) + 
  geom_point(alpha = 0.1, color = "#E46726") + 
  theme_minimal() +
  scale_x_continuous(trans='log10') +
  scale_y_continuous(trans='log10') +
  geom_smooth(method = "lm", se = TRUE, formula = y ~ x, color = "black", linetype = 'dashed') + 
  labs(x = "Views (Log 10 Scale)",
       y = "Likes (Log 10 Scale)",
       title = "Correlation of Likes vs. Views")
```

After transforming the data on both axes with a log 10 scale, we can see the very linear, positive relationship between the dependent variable Likes, and our strongest correlated independent variable, comment_count.

Next we'll create a simple linear regression model and a multiple linear regression model and test which performs better. 

```{r}
fit1 <- lm(formula = log1p(likes) ~ log1p(comment_count), data = df)

summary(fit1)
```

```{r}
fit2 <- lm(formula = log1p(likes) ~ log1p(views) + log1p(comment_count) + log1p(dislikes), data = df)

summary(fit2)
```


```{r}
anova(fit1, fit2)
```


**Summary:** Both models are significant with very small P-values, and each independent variable in the multiple-regression model is also significant.  When we perform an Anova test for significance, we can see that the Multiple Linear model (Fit2) does still hold up and therefore is the better model increasing the Adjusted R-squared from `0.6322` to `0.7637`.  

Therefore, with Model 2, we can say that for the dependent variable Like, *Views*, *Comment_Count*, and *Dislikes* contribute to `76.37%` of the variance. 

# Text Analysis

 * Text Analytics: perform the following at minimum, more if desired, but stay within the context of current learning
   * Perform necessary preprocessing
   * What words appear most in the titles and description in of trending videos (eg: most liked) ?

## Titles

```{r}
title_corpus <- Corpus(VectorSource(df$title))
title_corpus
```

```{r}
inspect(title_corpus[1:4])
```

## Preprocessing
Using the `tm` package, perform transformations on the corpus to clean the text. There are generalized text cleaning activities such as normalization and stop word removal.

```{r}
# standard cleansing
title_corpus <- tm_map(title_corpus, tolower)            # normalize case
title_corpus <- tm_map(title_corpus, removePunctuation)  # remove punctuation
title_corpus <- tm_map(title_corpus, removeNumbers)      # remove numbers
title_corpus <- tm_map(title_corpus, stripWhitespace)    # remove white space
title_corpus <- tm_map(title_corpus, removeWords, stopwords("english")) # remove stopwords
```

```{r}
inspect(title_corpus[1:4])
```

```{r}
# stem words using SnowBall stemmer
title_corpus <- tm_map(title_corpus, stemDocument)
```

## Document-Term Matrix
Create a `Term-Document Matrix` from the cleaned Corpus

```{r}
# The term document matrix is where each word/term is a row with documents as columns
dtm <- TermDocumentMatrix(title_corpus)

# inspect
inspect(dtm)
```

```{r}
dtm1 = removeSparseTerms(dtm, 0.99)
```

## Perform Analysis

### Frequent Terms
 * Use `freqwords()`: find frequent terms in a document-term or term-document matrix.
 * Find terms that occur at least 5 times and show top 50

```{r}
findFreqTerms(dtm1, 5) %>%
  head(50)
```

```{r}
termCount <- rowSums(as.matrix(dtm1))  # sums rows
termCount <- subset(termCount, termCount >=20)

df2 <- data.frame(term = names(termCount), freq = termCount) 
```

```{r}
df2 %>%
  head(35) %>%
  ggplot( aes(x = reorder(term, freq), y = freq, fill= freq)) + 
    geom_bar(stat = "identity") +
    scale_colour_gradientn(colors = terrain.colors(10)) + 
    theme_classic() +
    coord_flip() +
    theme(
    plot.title = element_text(face = "bold")) +
    labs(
    x = NULL, y = "Count",
    title = "Most Frequently Occuring Words in Titles"
    )
```

## Descriptions

```{r}
description_corpus <- Corpus(VectorSource(df$description))
description_corpus
```

```{r}
inspect(description_corpus[1:2])
```

## Preprocessing
Using the `tm` package, perform transformations on the corpus to clean the text. There are generalized text cleaning activities such as normalization and stop word removal.


```{r}
# Remove URLs
description_corpus <- tm_map(description_corpus, 
                             content_transformer(function(x) gsub("http[[:alnum:][:punct:]]*", "", x)))

# Replace new line symbols with a space
description_corpus <- tm_map(description_corpus, 
                             content_transformer(function(x) gsub("\\\\n", "", x)))

# Remove the odd "arrow" symbol
description_corpus <- tm_map(description_corpus, 
                             content_transformer(function(x) gsub("►", "", x)))

# Remove the odd "arrow" symbol
description_corpus <- tm_map(description_corpus, 
                             content_transformer(function(x) gsub("▶", "", x)))

```


```{r}
inspect(description_corpus[1:4])
```

```{r}
# standard cleansing
description_corpus <- tm_map(description_corpus, tolower)            # normalize case
description_corpus <- tm_map(description_corpus, removePunctuation)  # remove punctuation
description_corpus <- tm_map(description_corpus, removeNumbers)      # remove numbers
description_corpus <- tm_map(description_corpus, stripWhitespace)    # remove white space
description_corpus <- tm_map(description_corpus, removeWords, stopwords("english")) # remove stopwords
```



```{r}
inspect(description_corpus[1:4])
```

```{r}
# Use the Snowball Stemmer on the Corpus
description_corpus <- tm_map(description_corpus, stemDocument)
```

## Document-Term Matrix
Create a `Term-Document Matrix` from the cleaned Corpus

```{r}
# The term document matrix is where each word/term is a row with documents as columns
description_dtm <- TermDocumentMatrix(description_corpus)

# inspect
inspect(description_dtm)
```

```{r}
description_dtm1 = removeSparseTerms(description_dtm, 0.99)
```

### Frequent Terms
 * Use `freqwords()`: find frequent terms in a document-term or term-document matrix.
 * Find terms that occur at least 5 times and show top 50

```{r}
findFreqTerms(description_dtm1, 5) %>%
  head(50)
```

```{r}
termCount <- rowSums(as.matrix(description_dtm1))  # sums rows
termCount <- subset(termCount, termCount >=20)

description_df <- data.frame(term = names(termCount), freq = termCount) 
```

```{r}
description_df %>%
  head(35) %>%
  ggplot( aes(x = reorder(term, freq), y = freq, fill= freq)) + 
    geom_bar(stat = "identity") +
    scale_colour_gradientn(colors = terrain.colors(10)) + 
    theme_classic() +
    coord_flip() +
    theme(
    plot.title = element_text(face = "bold")) +
    labs(
    x = NULL, y = "Count",
    title = "Most Frequently Occuring Words in Titles"
    )
```







